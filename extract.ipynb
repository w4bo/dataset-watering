{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "# Replace these values with your database connection details\n",
    "db_params = {\n",
    "    'host': config[\"IN_HOST\"],\n",
    "    'database': config[\"IN_DB\"],\n",
    "    'user': config[\"IN_USER\"],\n",
    "    'password': config[\"IN_PWD\"],\n",
    "    'port': config[\"IN_PORT\"]\n",
    "}\n",
    "\n",
    "# Construct the connection string\n",
    "conn_str = \"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}\".format(**db_params)\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to select all data from the table\n",
    "sql_query = f\"\"\"\n",
    "select distinct\n",
    "\tf.\"name\" as field, s.\"name\" as scenario, s.description as scenario_description, s.scenario_arpae_name as arpae, s.scenario_water_name as watering, \n",
    "\td.unix_timestamp as sensor_timestamp, d.value_type_name as sensor_type, d.x as sensor_x, d.y as sensor_y, d.z as sensor_z, d.value as sensor_value\n",
    "from synthetic_data d, synthetic_field f, synthetic_scenario s\n",
    "where d.field_name = f.\"name\" and d.scenario_name = s.\"name\"\n",
    "\"\"\"\n",
    "\n",
    "# Use pandas to read the query result into a DataFrame\n",
    "df = pd.read_sql(sql_query, engine)\n",
    "\n",
    "# Now 'df' contains the data from the PostgreSQL table\n",
    "df.to_csv('data/sensor_ft.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to select all data from the table\n",
    "sql_query = f\"\"\"\n",
    "select distinct\n",
    "\tsf.field_name as field, s.\"name\" as scenario, s.description as scenario_description, s.scenario_arpae_name as arpae, s.scenario_water_name as watering,\n",
    "\tda.unix_timestamp as arpae_timestamp, da.value_type_name as arpae_type, da.value as arpae_value\n",
    "from synthetic_field_scenario sf, synthetic_scenario s, synthetic_scenario_arpae_data da\n",
    "where sf.scenario_name = s.\"name\" and s.scenario_arpae_name = da.scenario_arpae_name\n",
    "\"\"\"\n",
    "\n",
    "# Use pandas to read the query result into a DataFrame\n",
    "df = pd.read_sql(sql_query, engine)\n",
    "\n",
    "# Now 'df' contains the data from the PostgreSQL table\n",
    "df.to_csv('data/weather_ft.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to select all data from the table\n",
    "sql_query = f\"\"\"\n",
    "select distinct\n",
    "\tsf.field_name as field, s.\"name\" as scenario, s.description as scenario_description, s.scenario_arpae_name as arpae, s.scenario_water_name as watering, \n",
    "\tdw.unix_timestamp as water_timestamp, dw.value_type_name as water_type, dw.value as water_value\n",
    "from synthetic_field_scenario sf, synthetic_scenario s, synthetic_scenario_water_data dw\n",
    "where sf.scenario_name = s.\"name\" and s.scenario_water_name = dw.scenario_water_name\n",
    "\"\"\"\n",
    "\n",
    "# Use pandas to read the query result into a DataFrame\n",
    "df = pd.read_sql(sql_query, engine)\n",
    "\n",
    "# Now 'df' contains the data from the PostgreSQL table\n",
    "df.to_csv('data/irrigation_ft.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(file):\n",
    "    sdf = pd.read_csv(file)\n",
    "    sdf = sdf.rename({\"field\": \"fieldDesc\", \"watering\": \"wateringDesc\", \"arpae\": \"arpaeDesc\", \"scenario\": \"scenarioDesc\"}, axis=1)\n",
    "    sdf.columns = [x.replace(\"sensor_\", \"\").replace(\"arpae_\", \"\").replace(\"water_\", \"\") for x in sdf.columns]\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def get_last_4_digits(data):\n",
    "    # Compute the hash\n",
    "    hash_object = hashlib.sha256(data.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "\n",
    "    # Convert the hash to an integer and get the last 4 digits\n",
    "    last_4_digits = int(hash_hex, 16) % 10000000\n",
    "\n",
    "    return last_4_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_df(sdf):\n",
    "    sdf[\"field\"] = sdf.apply(lambda x: \"field-{}\".format(get_last_4_digits(x[\"fieldDesc\"] + \"-\" + x[\"scenarioDesc\"] + \"-\" + x[\"arpaeDesc\"])), axis=1)\n",
    "    sdf['datetime'] = pd.to_datetime(sdf['timestamp'], unit='s')\n",
    "    sdf['date'] = pd.to_datetime(sdf['timestamp'], unit='s').dt.date\n",
    "    sdf['month'] = pd.to_datetime(sdf['timestamp'], unit='s').dt.month\n",
    "    sdf['year'] = pd.to_datetime(sdf['timestamp'], unit='s').dt.year\n",
    "    sdf['month'] = sdf.apply(lambda x: \"{}-{}\".format(x[\"year\"], x[\"month\"]), axis=1)\n",
    "    sdf['hour'] = pd.to_datetime(sdf['timestamp'], unit='s').dt.hour\n",
    "    sdf['hour'] = sdf.apply(lambda x: \"{} {}:00:00\".format(x[\"date\"], x[\"hour\"]), axis=1)\n",
    "    sdf[\"agentType\"] = sdf.apply(lambda x: \"AssignedDevice\", axis=1)\n",
    "    sdf[\"timestampReceived\"] = sdf[\"timestamp\"]\n",
    "    sdf[\"delay\"] = sdf[\"timestampReceived\"] - sdf[\"timestamp\"] \n",
    "    sdf[\"province\"] = \"FE\"\n",
    "    sdf[\"region\"] = \"ER\"\n",
    "    sdf[\"country\"] = \"IT\"\n",
    "    sdf[\"owner\"] = \"Forecasting simulation\"\n",
    "\n",
    "sdf = read_df('data/sensor_ft.csv')\n",
    "extend_df(sdf)\n",
    "sdf[\"agent\"] = sdf.apply(lambda x: \"sensor-({},{},{})\".format(x[\"x\"], x[\"y\"], x[\"z\"]), axis=1)\n",
    "sdf[\"type-ext\"] = sdf.apply(lambda x: \"{}-({},{},{})\".format(x[\"type\"], x[\"x\"], x[\"y\"], x[\"z\"]), axis=1)\n",
    "sdf.to_csv('data/sensor_enr_ft.csv', index=False)\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf = read_df('data/weather_ft.csv')\n",
    "extend_df(wdf)\n",
    "wdf[\"agent\"] = \"WeatherStation\"\n",
    "wdf[\"type-ext\"] = wdf[\"type\"]\n",
    "wdf.to_csv('data/weather_enr_ft.csv', index=False)\n",
    "wdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = read_df('data/irrigation_ft.csv')\n",
    "extend_df(idf)\n",
    "idf[\"agent\"] = \"Dripper\"\n",
    "idf[\"type-ext\"] = idf[\"type\"]\n",
    "idf.to_csv('data/irrigation_enr_ft.csv', index=False)\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {\n",
    "    \"ft_measurement\": [\"agent\", \"type\", \"field\", \"owner\", \"timestamp\", \"value\", \"delay\"],\n",
    "    \"dt_field\": [\"field\", \"fieldDesc\", \"scenarioDesc\", \"scenario_description\", \"arpaeDesc\", \"wateringDesc\", \"province\", \"region\", \"country\"],\n",
    "    \"dt_time\": [\"timestamp\", \"datetime\", \"hour\", \"date\", \"month\", \"year\"],\n",
    "    \"dt_agent\": [\"agent\", \"agentType\"],\n",
    "}\n",
    "\n",
    "columns = list(set([item for sublist in [c for c in tables.values()] for item in sublist]))\n",
    "edf = pd.concat([sdf[columns], idf[columns], wdf[columns]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name2 = config[\"OUT_DB\"]\n",
    "def connect(db_name1):\n",
    "    # Connect to PostgreSQL server\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=db_name1,\n",
    "        user=config[\"OUT_USER\"],\n",
    "        password=config[\"OUT_PWD\"],\n",
    "        host=config[\"OUT_HOST\"],\n",
    "        port=config[\"OUT_PORT\"]\n",
    "    )\n",
    "    conn.autocommit = True\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "try: \n",
    "    conn = connect(config[\"IN_DB\"])\n",
    "    # Create a cursor\n",
    "    cursor = conn.cursor()\n",
    "    # Define the SQL command to create the database (if not exists)\n",
    "    create_db_query = sql.SQL(\"CREATE DATABASE {};\").format(sql.Identifier(db_name2))\n",
    "    # Execute the SQL command\n",
    "    cursor.execute(create_db_query)\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "except:\n",
    "    print(\"DB already exists\")\n",
    "\n",
    "db_params['database'] = db_name2\n",
    "conn_str = \"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}\".format(**db_params)\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "def get_type(x):\n",
    "    if x == \"timestamp\" or x == \"delay\":\n",
    "        return \"numeric\"\n",
    "    elif x == \"value\":\n",
    "        return \"double precision\"\n",
    "    else:\n",
    "        return \"varchar\"\n",
    "\n",
    "conn = connect(db_name2)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for tablename, columns in tables.items():\n",
    "    print(tablename)\n",
    "    try: \n",
    "        create_db_query = sql.SQL(\"DROP TABLE {};\").format(sql.Identifier(tablename))\n",
    "        cursor.execute(create_db_query)\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"Table {} does not exist\".format(tablename))\n",
    "    \n",
    "    try: \n",
    "        # create_db_query = sql.SQL(\"CREATE TABLE {} ({});\".format(tablename, ', '.join([\"{} {}\".format(x, get_type(x)) for x in columns])))\n",
    "        # print(create_db_query)\n",
    "        # cursor.execute(create_db_query)\n",
    "        # conn.commit()\n",
    "        edf[columns].drop_duplicates().to_sql(tablename, engine, if_exists='replace', index=False)\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "        print(\"FAIL to create {}\".format(tablename))\n",
    "\n",
    "\n",
    "for statement in [\n",
    "    \"ALTER TABLE dt_time ADD PRIMARY KEY (timestamp);\",\n",
    "    \"ALTER TABLE dt_field ADD PRIMARY KEY (field);\",\n",
    "    \"ALTER TABLE dt_agent ADD PRIMARY KEY (agent);\",\n",
    "    \"ALTER TABLE ft_measurement ADD PRIMARY KEY (timestamp, field, agent, type, owner);\",\n",
    "    \"ALTER TABLE ft_measurement ADD FOREIGN KEY (timestamp) REFERENCES dt_time(timestamp);\",\n",
    "    \"ALTER TABLE ft_measurement ADD FOREIGN KEY (field) REFERENCES dt_field(field);\",\n",
    "    \"ALTER TABLE ft_measurement ADD FOREIGN KEY (agent) REFERENCES dt_agent(agent);\"\n",
    "    ]:\n",
    "    try: \n",
    "        print(statement)\n",
    "        create_db_query = sql.SQL(statement)\n",
    "        cursor.execute(create_db_query)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {\n",
    "    \"ft_field_measurement\": [\"type-ext\", \"field\", \"owner\", \"timestamp\", \"value\"]\n",
    "}\n",
    "\n",
    "columns = list(set([item for sublist in [c for c in tables.values()] for item in sublist]))\n",
    "rdf = pd.concat([sdf[columns], idf[columns], wdf[columns]], ignore_index=True)\n",
    "rdf = rdf.rename({\"type-ext\": \"type\"}, axis=1)\n",
    "pivoted_df = rdf[[\"field\", \"timestamp\", \"type\", \"owner\", \"value\"]].pivot(index=[\"field\", \"timestamp\", \"owner\"], columns='type', values='value')\n",
    "pivoted_df.reset_index(inplace=True)\n",
    "pivoted_df = pivoted_df.dropna(axis=1, how='all').fillna(method='ffill').fillna(method='bfill').dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(db_name2)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "db_params['database'] = db_name2\n",
    "conn_str = \"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}\".format(**db_params)\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "tablename = \"ft_field_measurement\"\n",
    "try: \n",
    "    create_db_query = sql.SQL(\"DROP TABLE {};\").format(sql.Identifier(tablename))\n",
    "    cursor.execute(create_db_query)\n",
    "    conn.commit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try: \n",
    "    pivoted_df.drop_duplicates().to_sql(tablename, engine, if_exists='replace', index=False)\n",
    "except Exception as e:\n",
    "    print(e) \n",
    "    print(\"FAIL to create {}\".format(tablename))\n",
    "\n",
    "\n",
    "for statement in [\n",
    "    \"ALTER TABLE ft_field_measurement ADD PRIMARY KEY (timestamp, field, owner);\",\n",
    "    \"ALTER TABLE ft_field_measurement ADD FOREIGN KEY (timestamp) REFERENCES dt_time(timestamp);\",\n",
    "    \"ALTER TABLE ft_field_measurement ADD FOREIGN KEY (field) REFERENCES dt_field(field);\",\n",
    "    ]:\n",
    "    try: \n",
    "        print(statement)\n",
    "        create_db_query = sql.SQL(statement)\n",
    "        cursor.execute(create_db_query)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
